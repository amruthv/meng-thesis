\chapter{Existing Motion Planning Techniques}
In this chapter, we look at an overview of recent developments in motion planning. We go over their advantages and limitations. Since most motion planning algorithms incorporate randomness, with some probability they will at some point perform worse than other algorithms. Consequently, we are interested in how often they perform well to measure their robustness.

\section{Traditional Motion Planning Techniques} \label{planning:techniques}
We look at two classes of traditional planners: multi-query and single-query. The planning techniques discussed in this section only look for feasible paths --- those without collisions.

\subsection{Multi-query Planners}
Multi-query planners are meant to support multiple queries for paths between start and goal configurations in the same environment. This means that the parameters of the world are unchanging, including the obstacles, their location, and the size of the world. The multiple query nature of the planner means the planner is designed and optimized to build a data structure that will prove useful regardless of the specified start and goal configurations for a planning instance. This can mean that prior to even computing any paths, the planner must spend time on computation to accumulate this useful information.

\subsubsection{Probabilistic Roadmap}
The probabilistic roadmap (PRM) is one such multi-query planner \cite{kavraki:prm}. The PRM works in two phases - the learning phase and the query phase. The learning phase works by initializing a graph $G=(V,E)$ and growing it with a twofold process. 

The first step is the construction step in which we sample points and connect them. Specifically, we sample a random configuration $c$ from free space and add it to $G$. Then using a selection strategy, we choose some nodes from $V$ to attempt connection with $c$. If the direct path from $c$ to each of these nodes is collision free, add this edge to $E$. The second step is the expansion step which helps achieve good exploration in "difficult" areas. We do this by sampling points with a distribution proportional to the need to explore each area, which is approximated by a heuristic. Then given one of these vertices, we choose an arbitrary direction and extend in that direction. Whenever an obstacle is hit, we pick another random direction and repeat this with some limit. Finally this final configuration is connected similarly in the construction step. 

By the end of the learning phase, the PRM is ready for querying. As soon as a path is found from each of the specified start and goal configurations to a vertex in $G$, a path can be constructed for the entire trajectory that is already known to be collision free. This is helpful as it can reduce the number of expensive collision checks that need to be done for a particular planning problem. 

The PRM is an effective method for multi-queries because it leverages the fact that the world does not change, so a collision free trajectory at this instant will be collision free at a future time as well. However, this very strength is a limitation, as it can't be used efficiently in a planner that moves obstacles. The time spent pre-computing trajectories will only be useful that one time and can provide no further benefit.

\subsection{Single-query Planners}
Single-query planners are valuable tools in scenarios where we expect the world to change over time, either because obstacles are moving or dynamics of the robot are changing (perhaps the robot is now holding something). Since we do not benefit from pre-computation, a single-query planner must be efficient at answering a particular planning question. Below we discuss advances made in this sub-field of single-query planners that are still relevant today.

\subsubsection{Rapidly-Exploring Random Trees} \label{planning:rrt}
The rapidly-exploring random tree (RRT) is a randomized data structure that was designed with few tunable parameters and heuristics \cite{lavalle:rrt}. The holonomic variant of the procedure is shown in Algorithm \ref{alg:rrt}.

{\singlespacing{
\begin{algorithm}
\caption{RRT}
\label{alg:rrt}
\begin{algorithmic}[1]
\Function{Generate RRT}{$x_{init}$, $K$}
\State $T$.init($x_{init}$)
\For{k = 1 to $K$}
\State $x_{rand} \gets$ \Call{Random Sample}{}
\State $x_{near} \gets$ \Call{Nearest}{$x_{rand}, T$}
\State $x_{new} \gets$ \Call{Stopping Configuration}{$x_{near}, x_{rand}$}
\State $T$.add\_vertex($x_{new}$)
\State $T$.add\_edge($x_{near}, x_{new}$)
\EndFor
\State Return $T$
\EndFunction
\end{algorithmic}
\end{algorithm}
}}

This data structure grows by iteratively sampling from a configuration space. It then finds the closest data point $x_{near}$ currently in our tree data structure $T$. Given these two configurations, we attempt to find a stopping configuration that is an extension in the direction from $x_{near}$ to $x_{rand}$. This procedure performs collision checking along the vector to maintain the invariant that any vertices and edges added to $T$ are collision free. If a collision free stopping configuration $x_{new}$ is found, we augment $T$ wit an edge from $x_{near}$ to $x_{new}$. To use the RRT in a motion planning scenario where we plan from $x_1$ to $x_2$, we simply instantiate the RRT with $x_{init} = x_1$ and check if $x_{new}$ is sufficiently close to $x_2$. If such an $x_2$ is found, then a path can be reconstructed using $T$.

The RRT has some nice properties. First, the RRT is likely to explore unexplored areas in the configuration space given a reasonable sampling strategy (e.g. uniform sampling). In the case of uniform sampling, the probability of an existing configuration being expanded is proportional to the size of its corresponding Voronoi region. Because the largest Voronoi regions are on the frontier of searched space, the tree will grow towards unexplored area. Second, the distribution in the RRT data structure approaches the sampling distribution as the RRT is grown. The last and arguably most important aspect is that, like the PRM, the RRT is probabilistically complete, meaning that the probability that a solution is found (if it exists) approaches 1 as the amount of time spent on the RRT plan increases. A very significant drawback of this algorithm is that this algorithm \emph{cannot} determine that there does not exist a solution. It can try indefinitely to find a solution.

\subsubsection{Goal Biased RRT} \label{planning:goal-bias}
While the aforementioned RRT motion planning algorithm is probabilistically complete, it does not provide useful bounds on how \emph{fast} it can find a solution given that one exists. As the RRT name suggests, it randomly explores the state space, eventually exploring the entire space in the limit. In a motion planning setting we know exactly where we want a path to terminate, namely the goal. A simple, yet powerful idea was to modify the sampling strategy to include a goal bias \cite{lavalle:rrt}. That is, with some small probability $p$ we "sample" the goal and with probability $1-p$ we sample from the original strategy. The choice of $p$ affects the prioritization of state space exploration versus greedy search. A lower $p$ defers to the standard RRT, with $p=0$ being exactly the original RRT. A higher value of $p$ will stretch the current tree towards the goal and more strongly guide the expansion. Goal biasing is often a good choice since in single-query planning we are interested in some path rather than a complete understanding of the reachability of the space.

\subsubsection{Bidirectional RRT} \label{planning:birrt}
The bidirectional RRT method was proposed by Kuffner and Lavalle in 2005 as a means to speed up the RRT algorithm \cite{lavalle:birrt}. It is based on a similar principle as the goal biased RRT in that we can guide the direction of the growth of the RRT. However, instead of growing a single tree, we can instantiate two RRTs and help them grow towards each other. Doing so can help the planner find solutions more quickly, as in the case when there are bug traps in the configuration space \cite{lavalle:planning}. To encourage the rapidly exploring nature of the RRT, the bi-RRT simply maintains that the difference between tree sizes never gets too large. The algorithm pseudocode is shown in Algorithm \ref{alg:birrt}.

{\singlespacing{
\begin{algorithm}[t]
\caption{Bidirectional RRT}
\label{alg:birrt}
\begin{algorithmic}[1]
\Function{ConstructBiRRT}{$q_s,q_g$}
\State $T_a$.init($q_s$)
\State $T_b$.init($q_g$)
\For{k = 1 to $K$}
    \State $q_s \gets$ \Call{Random Sample}{}
    \State $q_{near} \gets$ \Call{Nearest}{$T_a, q_s$}
    \State $q_n \gets$ \Call{Stopping Configuration}{$q_{near}, q_s$}
    \If{$q_{near} \ne q_n$}
        \State $T_a$.add\_vertex($q_n$)
        \State $T_a$.add\_edge($q_{near}, q_n$)
        \State $q_{near}' \gets$ \Call{Nearest}{$T_b, q_n$}
        \State $q_n' \gets$ \Call{Stopping Configuration}{$q_{near}', q_n$}
        \If{$q_{near}' \ne q_n'$}
            \State $T_b$.add\_vertex($q_n'$)
            \State $T_b$.add\_edge($q_{near}', q_n'$)
        \EndIf
        \If{$q_n = q_n'$}
            \State Return SOLUTION
        \EndIf
    \EndIf
    \If{$|T_a| > |T_b|$}
        \State SWAP($T_a, T_b$)
    \EndIf
\EndFor
\State Return FAILURE
\EndFunction
\end{algorithmic}
\end{algorithm}
}}

\section{The MCR Algorithm}
In this section we describe the Minimum Constraint Removal (MCR) algorithm as formulated by Hauser (for the formal problem statement see section \ref{intro:statement}). The discussion that follows draws heavily from Hauser's description \cite{hauser:mcr}. MCR is the first algorithm we discuss that tolerates collisions while searching for paths. We go into a more extensive description and analysis of MCR as we are interested in the shortcomings of this algorithm.

\subsection{Methods of Solving the MCR Problem} \label{mcr:solving}
Hauser proposed two types of solutions employing classic search techniques: an exact search method (best first search) and a close approximation that should generally run faster (greedy search).

\subsubsection{Best First Search}
This method is guaranteed to eventually find $S^{*}$. In this method, states are tuples of configuration and covers $(v, S_v)$, where $S_v$ is the cover of some path from $s \rightarrow v$. There are many such tuples for every node $v$ in the graph. We search through the graph beginning with the start node and perform best-first search using the size of the covers as the distance metric. To expand from $a$ to $b$, for two such connected vertices, we set $S_b = S_a \bigcup C[b]$. Because we maintain all possible covers for each vertex, we guarantee that the minimum constraint removal will be found. For a world with $m$ obstacles, since each configuration can have $2^m$ different covers, the total state space is $O(|E|2^m)$, making this approach infeasible.

\subsubsection{Greedy Search}
Greedy search operates much in the way that best first search does except in the enumeration of states. Rather than keeping all possible $2^m$ covers per node, we instead only maintain a cover that has the lowest size. When we expand the graph from $a$ to $b$, we re-evaluate the minimum cover $S_b$ and then update the covers of the neighbors of $b$ as necessary. Since each node is now only expanded once, the runtime is now reduced to $O(|E|m)$. In fact, if the configurations for which $O_i$ is in their covers form a connected subsequence for the found path for all $O_i \in S_g$, then $S_g = S^{*}_g$. This will often be the case --- greedy search is often as good as exact search.

\subsection{MCR Planner Details}
\subsubsection{Approximating Connectivity}
In the prior discussion, we assumed that we were already provided a graph $G$ consisting of configurations for a robot. In practice, we must construct this ourselves. We want $G$ to accurately approximate the connectivity of the configuration space, where connectivity is a function of how collision-free the space is. That is, the less cluttered the configuration space is, the more connected the space is. To formalize this notion of reachability we use $(G,k)$ reachability.

{\bf{Definition}}: A node $q$ is $(G,k)$ reachable if there is a path from a start node $s$ to $q$ with a cover at most $k$. 

The MCR planner grows $G$ in the fashion of a PRM. We repeatedly sample points randomly in our configuration space and attempt to connect them to nearby points specified by some distance metric. We also incorporate techniques from rapidly exploring random graphs (RRGs) which prioritize rapid exploration of $(G,k)$ reachable space. Specifically, when sampling a point and looking for points to connect it to, we consider only those that are $(G,k)$ reachable. This ensures that we are expanding from the \emph{exploration limit}. This way we can expand the connectivity of our $(G,k)$ reachable space. The combination of these techniques improves the connectivity of $G$ and the approximation of the real connectivity. 

By examining the psueodocode in Algorithm \ref{alg:mcr}, we see that we increment $k$ every $N_{raise}$ steps ($N_{raise}$ can be chosen as needed based on the size of the problem). This ensures that we get good connectivity for a particular exploration limit before trying to search for less accessible configurations. Having good connectivity for lower $k$ helps improve the accuracy of the approximation to the real connectivity. 

\subsubsection{Weighted MCR}
The weighted MCR problem can be trivially extended from the original formulation by defining the minimum constraint removal to be the minimum sum of weights for obstacles in the cover of a best path from the start to goal configuration. We can further emulate immovable obstacles by assigning them "infinite" weight such that the MCR planner will never plan a path through these obstacles if possible. 

Throughout this paper, we will use the notation $|S|$ to refer to the cover size in both the unweighted and weighted constraint removal problem; from the planning problem it is clear whether this refers to the cardinality of $S$ or the sum of the weighted obstacles in $S$. 

\subsubsection{Implementation Specifics}
The pseudocode in Algorithm \ref{alg:mcr} is largely similar to Hauser's original MCR formulation. We introduce an explicit exit condition for the MCR algorithm and modify the \textproc{Extend Toward} to fail if the original extension fails. 

%\begin{figure}[H]
{\singlespacing{
\begin{algorithm}[t]
\caption{Minimum Constraint Removal}
\label{alg:mcr}
\begin{algorithmic}[1]
\Function{MCR}{$q_s, q_g$}
\State $S_{min} \gets$ \Call{EdgeCover}{$q_s, q_g$} 
\State $k \gets |$Cover$(q_s) \bigcup$ Cover$(q_g)|$ 
\State $G \equiv (V,E) \gets ({q_s \rightarrow q_g})$
\While{iteration count < $N_{raise} \times (S_{min} + tolerance)$}
\State \Call{Expand Roadmap}{$G,k$}
\State Compute the minimum explanations $S_G(q)$ for all $q \in V$
\State $S_{min} \gets S_g$
\State Every $N_{raise}$ step, $k \gets k + 1$
\If{$k \ge |S_{min}|$}
    \State $k \gets |S_{min}| - 1$
\EndIf
\EndWhile
\EndFunction
\\
\Function{Expand Roadmap}{$G,k$}
\State $q_d \gets Sample()$
\State Let $q_n \gets$ \Call{Closest}{$G,k, q_d$}
\State $q \gets$ \Call{Extend Toward}{$q_n, q_d, \delta, k$}
\If{$q$ is not $None$}
    \State Let ${q_1, q_2, ..., q_n} \gets$ \Call{Neighbors}{$G,q$}
    \For {$i = 1,2,....n$}
    \If{$d(q_i,q) < \delta$}
    \State Add $q_i \rightarrow q$ to $E$
    \EndIf
    \EndFor
\EndIf
\EndFunction
\\
\Function{Closest}{$G,k,q$}
\State Return $\argmin\limits_{q_i \in V} d(q, q_i)$
\EndFunction
\\
\Function{Neighbors}{$G,q$}
\State $Distances \gets []$
\For {$i = 1,2, ..., |V|$}
\State $Distances \gets Distances + [(d(q, q_i), i)]$
\EndFor
\State Sort($Distances$)
\State Return first $m$ neighbors in $Distances$
\EndFunction
\\
\Function{Extend Toward}{$q_i, q, \delta, k$}
\State $q' \gets q_i + min(\frac{\delta}{d(q_i, q)}, 1)(q- q_i)$
\State Return $q'$ if $|S_q \bigcup$ \Call{EdgeCover}{$q_i,q'$}$| <= k$ else $None$
\EndFunction
\end{algorithmic}
\end{algorithm}
}}

The MCR algorithm follows a straightforward loop. In the initialization, we compute an upper bound on $S^{*}_g$ since $S^{*}_g$ can be no worse than the straight path from $s \rightarrow g$. Next we set $k$ to the union of the covers at $s$ and $g$ since this is at most the initial $S_{min}$ value. Then we initialize our graph and begin the task of growing and measuring connectivity in lines 5-11. 

The subroutine \textproc{Expand Roadmap} first generates a random configuration $q_d$. It then finds the closest $(G,k)$ reachable node $q_n$. Similar to RRT growth, we extend $q_n$ towards $q_d$ up to some distance tolerance $\delta$. If this new configuration $q'$ isn't within the $k$ exploration limit, we abandon and restart the for loop of MCR. Now that we have a new vertex $q'$ to add to $G$, we choose neighbor candidates. As mentioned before, we incorporate the RRG strategy and connect $q'$ to up to $m$ of the nearest configurations as long as they are at most $\delta$ away \cite{karaman:rrg}. By construction in \textproc{Extend Toward}, we know that at least one neighbor will be within $\delta$ ($q_n$). 

Finally, given our expanded graph, we can compute the minimum explanations (the minimum $(G,k)$ reachabilities) for all nodes. This is exactly the procedure described in Section \ref{mcr:solving}. Since speed is primarily what we are concerned with (at the cost of complete correctness), we use the greedy method.

While we increment $k$ every $N_{raise}$ steps, we cap it to be one less than $S_{min}$, as it is unhelpful to raise our exploration limit higher than what we know is necessary. 

While the algorithm seems simple, there are some subtleties that help it succeed. We have emphasized the importance of respecting the exploration limit $k$. When we look for candidate nodes to connect a sample to, we only consider those that are $(G,k)$ reachable since we want to improve our estimation of the connectivity of $(G,k)$ reachable space. However, the next procedure \textproc{Neighbors}($G,q$) does not consider $k$. This appears to be inconsistent with the goal of maintaining the $k$ frontier, as we would now connect $q'$ to nodes that are not necessarily contained within the limit. However, this is essential to \emph{improving} $(G,k)$ reachability.

\subsection{MCR algorithm benefits and disadvantages}
The MCR algorithm succeeds where traditional planning fails. RRTs and other strategies can search forever looking for a path from a start configuration to a goal configuration. Until such a path is found, there is no deliverable progress. Hauser's MCR technique does not suffer from this weakness because it is an \emph{any-time} algorithm, meaning the algorithm can be terminated at any point and the minimum cover path thus far can be reconstructed. A downside is that in many cases, the cover of a best path is equal to the weight of the union of the two covers at the start and goal configurations; this is the solution that MCR starts with. However, it is not clear that it can terminate instead of continuing to iterate. This is a problem of looking for paths that tolerate some collisions -- the cover of some best path is not known a priori. We instead must assume that after some period of looking for better paths, if none is found, there is no better path. For this reason we modify the original formulation of MCR to run for a number of iterations that is an increasing linear function of the best path cover found thus far.

Another difficulty the MCR algorithm faces is in its frontier expansion strategy. If a best path from the start to goal configurations has a high weight cover of $w_h$, then until the MCR algorithm goes through $N_{raise} * w_h$ expansion steps (assuming a collision-free start and goal) this path can never be found. In fact, if we were to enumerate all possible unique covers, any $k$ considered between these values are unhelpful to finding paths. It helps to build a more dense graph, but this dense graph can be limited in its exploration of the configuration space and, depending on the distribution of obstacle weights, may spend excessive time developing these frontiers.

